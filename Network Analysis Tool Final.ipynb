{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec28868-4ff2-41c6-bd82-a8fb92c660d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete. Security report saved to: C:\\Users\\Service Casket\\Desktop\\Network Analysis Tool\\output\\security_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "import community.community_louvain as community_louvain\n",
    "from collections import Counter\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NetworkAnalysisTool:\n",
    "    def __init__(self, base_path):\n",
    "        \"\"\"Initialize the Network Analysis Tool\"\"\"\n",
    "        self.base_path = os.path.expanduser(base_path)\n",
    "        self.output_path = os.path.join(self.base_path, 'output')\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(self.output_path, 'analysis.log'),\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        self.load_data()\n",
    "        self.create_graph()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load nodes and edges data\"\"\"\n",
    "        try:\n",
    "            self.nodes_df = pd.read_csv(os.path.join(self.base_path, 'ICS_OT Nodes.csv'))\n",
    "            self.edges_df = pd.read_csv(os.path.join(self.base_path, 'ICS_OT Edges.csv'))\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Error loading CSV files: {e}\")\n",
    "            \n",
    "    def create_graph(self):\n",
    "        \"\"\"Create the network graph\"\"\"\n",
    "        self.G = nx.Graph()\n",
    "        \n",
    "        for _, row in self.nodes_df.iterrows():\n",
    "            self.G.add_node(row['Id'], label=row['Label'], shape=row.get('Shape', 'ellipse'))\n",
    "            \n",
    "        for _, row in self.edges_df.iterrows():\n",
    "            self.G.add_edge(row['Source'], row['Target'])\n",
    "\n",
    "    def analyze_network_structure(self):\n",
    "        \"\"\"Analyze basic network structure\"\"\"\n",
    "        logging.info(\"Analyzing network structure\")\n",
    "        \n",
    "        components = list(nx.connected_components(self.G))\n",
    "        cycles = list(nx.cycle_basis(self.G))\n",
    "        endpoints = [node for node, degree in dict(self.G.degree()).items() if degree == 1]\n",
    "        \n",
    "        self.centrality_measures = {\n",
    "            'Degree_Centrality': nx.degree_centrality(self.G),\n",
    "            'Betweenness_Centrality': nx.betweenness_centrality(self.G),\n",
    "            'Closeness_Centrality': nx.closeness_centrality(self.G),\n",
    "            'Eigenvector_Centrality': nx.eigenvector_centrality(self.G, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'components': components,\n",
    "            'cycles': cycles,\n",
    "            'endpoints': endpoints,\n",
    "            'density': nx.density(self.G),\n",
    "            'is_tree': nx.is_tree(self.G),\n",
    "            'is_forest': nx.is_forest(self.G)\n",
    "        }\n",
    "\n",
    "    def analyze_bottlenecks(self):\n",
    "        \"\"\"Analyze network bottlenecks using spectral properties\"\"\"\n",
    "        laplacian = nx.laplacian_matrix(self.G).todense()\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n",
    "        \n",
    "        n_components = 3\n",
    "        bottleneck_scores = {}\n",
    "        \n",
    "        for node in self.G.nodes():\n",
    "            idx = list(self.G.nodes()).index(node)\n",
    "            score = 0\n",
    "            for i in range(1, n_components + 1):\n",
    "                weight = 1 / i\n",
    "                score += weight * abs(eigenvectors[idx, i])\n",
    "                \n",
    "            degree = self.G.degree(node)\n",
    "            if degree > 0:\n",
    "                score = score / np.sqrt(degree)\n",
    "                \n",
    "            bottleneck_scores[node] = score\n",
    "        \n",
    "        threshold = np.percentile(list(bottleneck_scores.values()), 75)\n",
    "        critical_bottlenecks = {\n",
    "            node: score for node, score in bottleneck_scores.items() \n",
    "            if score > threshold\n",
    "        }\n",
    "        \n",
    "        n_walks = 1000\n",
    "        walk_length = 5\n",
    "        flow_counts = {node: 0 for node in self.G.nodes()}\n",
    "        \n",
    "        for _ in range(n_walks):\n",
    "            for start in self.G.nodes():\n",
    "                current = start\n",
    "                for _ in range(walk_length):\n",
    "                    neighbors = list(self.G.neighbors(current))\n",
    "                    if not neighbors:\n",
    "                        break\n",
    "                    current = np.random.choice(neighbors)\n",
    "                    flow_counts[current] += 1\n",
    "        \n",
    "        max_flow = max(flow_counts.values())\n",
    "        flow_centrality = {\n",
    "            node: count/max_flow \n",
    "            for node, count in flow_counts.items()\n",
    "        }\n",
    "        \n",
    "        bottleneck_analysis = {}\n",
    "        for node in self.G.nodes():\n",
    "            label = self.G.nodes[node]['label']\n",
    "            bottleneck_analysis[label] = {\n",
    "                'spectral_score': bottleneck_scores[node],\n",
    "                'flow_centrality': flow_centrality[node],\n",
    "                'is_critical': node in critical_bottlenecks,\n",
    "                'degree': self.G.degree(node),\n",
    "                'betweenness': self.centrality_measures['Betweenness_Centrality'][node]\n",
    "            }\n",
    "        \n",
    "        return bottleneck_analysis\n",
    "\n",
    "    def calculate_node_criticality(self):\n",
    "        \"\"\"Calculate node criticality based on network segmentation\"\"\"\n",
    "        original_components = nx.number_connected_components(self.G)\n",
    "        criticality = {}\n",
    "        \n",
    "        for node in self.G.nodes():\n",
    "            G_temp = self.G.copy()\n",
    "            G_temp.remove_node(node)\n",
    "            new_components = nx.number_connected_components(G_temp)\n",
    "            criticality[node] = new_components - original_components\n",
    "            \n",
    "        return criticality\n",
    "\n",
    "    def calculate_security_metrics(self):\n",
    "        \"\"\"Calculate ICS/SCADA-specific security metrics\"\"\"\n",
    "        control_systems = ['PLC 1', 'PLC 2', 'PLC 3', 'PLC 4', 'PLC 5']\n",
    "        hmi_systems = ['HMI 1', 'HMI 2', 'HMI 3']\n",
    "        field_devices = ['Sensor 1', 'Actuator 1'] + [f'Device {i}' for i in range(1, 12)]\n",
    "        \n",
    "        plc_nodes = [node for node, attr in self.G.nodes(data=True) if attr['label'] in control_systems]\n",
    "        hmi_nodes = [node for node, attr in self.G.nodes(data=True) if attr['label'] in hmi_systems]\n",
    "        field_nodes = [node for node, attr in self.G.nodes(data=True) if attr['label'] in field_devices]\n",
    "        \n",
    "        metrics = {}\n",
    "        for node in self.G.nodes():\n",
    "            metrics[node] = self._calculate_node_security_metrics(\n",
    "                node, plc_nodes, hmi_nodes, field_nodes,\n",
    "                control_systems, hmi_systems, field_devices\n",
    "            )\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "    def _calculate_node_security_metrics(self, node, plc_nodes, hmi_nodes, field_nodes,\n",
    "                                       control_systems, hmi_systems, field_devices):\n",
    "        \"\"\"Calculate security metrics for a single node\"\"\"\n",
    "        node_label = self.G.nodes[node]['label']\n",
    "        \n",
    "        min_path_to_plc = float('inf')\n",
    "        min_path_to_hmi = float('inf')\n",
    "        \n",
    "        for plc in plc_nodes:\n",
    "            if nx.has_path(self.G, node, plc):\n",
    "                min_path_to_plc = min(min_path_to_plc, nx.shortest_path_length(self.G, node, plc))\n",
    "                \n",
    "        for hmi in hmi_nodes:\n",
    "            if nx.has_path(self.G, node, hmi):\n",
    "                min_path_to_hmi = min(min_path_to_hmi, nx.shortest_path_length(self.G, node, hmi))\n",
    "                \n",
    "        return {\n",
    "            'plc_connections': sum(1 for n in self.G.neighbors(node) if n in plc_nodes),\n",
    "            'hmi_connections': sum(1 for n in self.G.neighbors(node) if n in hmi_nodes),\n",
    "            'field_connections': sum(1 for n in self.G.neighbors(node) if n in field_nodes),\n",
    "            'min_path_to_plc': min_path_to_plc,\n",
    "            'min_path_to_hmi': min_path_to_hmi,\n",
    "            'is_control_system': int(node_label in control_systems),\n",
    "            'is_hmi': int(node_label in hmi_systems),\n",
    "            'is_field_device': int(node_label in field_devices)\n",
    "        }\n",
    "\n",
    "    def detect_anomalies(self, security_metrics):\n",
    "        \"\"\"Enhanced anomaly detection with ICS-specific features\"\"\"\n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        for measure, values in self.centrality_measures.items():\n",
    "            features[measure] = pd.Series(values)\n",
    "            \n",
    "        for node in self.G.nodes():\n",
    "            for metric, value in security_metrics[node].items():\n",
    "                if metric not in features:\n",
    "                    features[metric] = 0\n",
    "                features.loc[node, metric] = float(value if value != float('inf') else 1000)\n",
    "                \n",
    "        features['plc_path_factor'] = features['min_path_to_plc'].apply(\n",
    "            lambda x: 1/(x+1) if x < 1000 else 0)\n",
    "        features['hmi_path_factor'] = features['min_path_to_hmi'].apply(\n",
    "            lambda x: 1/(x+1) if x < 1000 else 0)\n",
    "            \n",
    "        features['exposure_score'] = (\n",
    "            features['plc_connections'] * 3 +\n",
    "            features['hmi_connections'] * 2 +\n",
    "            features['field_connections'] +\n",
    "            features['plc_path_factor'] * 2 +\n",
    "            features['hmi_path_factor'] * 1.5\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        iso_forest = IsolationForest(contamination=0.1, n_estimators=100, random_state=42)\n",
    "        features['anomaly'] = iso_forest.fit_predict(features_scaled)\n",
    "        features['anomaly_score'] = iso_forest.score_samples(features_scaled)\n",
    "        \n",
    "        min_score = features['anomaly_score'].min()\n",
    "        max_score = features['anomaly_score'].max()\n",
    "        features['risk_score'] = 100 * (features['anomaly_score'] - max_score) / (min_score - max_score)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def calculate_spectral_metrics(self):\n",
    "        \"\"\"Calculate spectral metrics including spectral radius and Fiedler vector\"\"\"\n",
    "        adjacency_matrix = nx.adjacency_matrix(self.G).todense()\n",
    "        laplacian_matrix = nx.laplacian_matrix(self.G).todense()\n",
    "        \n",
    "        lap_eigenvalues, lap_eigenvectors = np.linalg.eigh(laplacian_matrix)\n",
    "        \n",
    "        spectral_radius = max(abs(lap_eigenvalues))\n",
    "        \n",
    "        idx = lap_eigenvalues.argsort()\n",
    "        fiedler_value = lap_eigenvalues[idx[1]]\n",
    "        fiedler_vector = lap_eigenvectors[:, idx[1]]\n",
    "        \n",
    "        node_labels = list(self.G.nodes())\n",
    "        fiedler_components = list(zip(node_labels, fiedler_vector))\n",
    "        fiedler_components.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'spectral_radius': spectral_radius,\n",
    "            'fiedler_value': fiedler_value,\n",
    "            'fiedler_components': fiedler_components\n",
    "        }\n",
    "\n",
    "    def visualize_risk_levels(self, risk_scores):\n",
    "        \"\"\"Create network visualization with risk levels\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        pos = nx.spring_layout(self.G, k=2, iterations=50)\n",
    "        \n",
    "        risk_colors = []\n",
    "        node_sizes = []\n",
    "        for node in self.G.nodes():\n",
    "            risk_score = risk_scores.loc[node, 'risk_score']\n",
    "            \n",
    "            if risk_score >= 75:\n",
    "                color = 'red'\n",
    "                size = 3000\n",
    "            elif risk_score >= 50:\n",
    "                color = 'orange'\n",
    "                size = 2500\n",
    "            elif risk_score >= 25:\n",
    "                color = 'yellow'\n",
    "                size = 2000\n",
    "            else:\n",
    "                color = 'green'\n",
    "                size = 1500\n",
    "                \n",
    "            risk_colors.append(color)\n",
    "            node_sizes.append(size)\n",
    "            \n",
    "        nx.draw_networkx_edges(self.G, pos, edge_color='gray', alpha=0.5)\n",
    "        nodes = nx.draw_networkx_nodes(self.G, pos,\n",
    "                                     node_color=risk_colors,\n",
    "                                     node_size=node_sizes)\n",
    "                                     \n",
    "        labels = nx.get_node_attributes(self.G, 'label')\n",
    "        nx.draw_networkx_labels(self.G, pos, labels, font_size=8)\n",
    "        \n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                    label=f'{level} Risk',\n",
    "                                    markerfacecolor=color, markersize=10)\n",
    "                          for level, color in [('Critical', 'red'),\n",
    "                                             ('High', 'orange'),\n",
    "                                             ('Moderate', 'yellow'),\n",
    "                                             ('Low', 'green')]]\n",
    "                                             \n",
    "        plt.legend(handles=legend_elements, loc='upper left',\n",
    "                  title='Risk Levels', bbox_to_anchor=(1, 1))\n",
    "                  \n",
    "        plt.title('Network Risk Level Visualization')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(os.path.join(self.output_path, 'risk_level_visualization.png'),\n",
    "                    bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def visualize_bottlenecks(self, bottleneck_analysis):\n",
    "        \"\"\"Create visualization of network bottlenecks\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        pos = nx.spring_layout(self.G, k=2, iterations=50)\n",
    "        \n",
    "        scores = [bottleneck_analysis[self.G.nodes[node]['label']]['spectral_score'] \n",
    "                 for node in self.G.nodes()]\n",
    "    \n",
    "        norm = plt.Normalize(vmin=min(scores), vmax=max(scores))\n",
    "        node_colors = plt.cm.YlOrRd(norm(scores))\n",
    "    \n",
    "        flow_centrality = [bottleneck_analysis[self.G.nodes[node]['label']]['flow_centrality'] * 3000 \n",
    "                          for node in self.G.nodes()]\n",
    "    \n",
    "        nx.draw_networkx_edges(self.G, pos, edge_color='gray', alpha=0.5, ax=ax)\n",
    "        nodes = nx.draw_networkx_nodes(self.G, pos, \n",
    "                                     node_color=node_colors,\n",
    "                                     node_size=flow_centrality,\n",
    "                                     ax=ax)\n",
    "    \n",
    "        labels = nx.get_node_attributes(self.G, 'label')\n",
    "        nx.draw_networkx_labels(self.G, pos, labels, font_size=8, ax=ax)\n",
    "    \n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.YlOrRd, norm=norm)\n",
    "        plt.colorbar(sm, ax=ax, label='Bottleneck Score')\n",
    "    \n",
    "        ax.set_title('Network Bottleneck Analysis')\n",
    "        ax.axis('off')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_path, 'bottleneck_analysis.png'),\n",
    "                    bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _analyze_zone_isolation(self, zone, security_metrics):\n",
    "        \"\"\"Analyze potential zone isolation violations\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        zone_nodes = {\n",
    "            'Control': [n for n in self.G.nodes() if self.G.nodes[n]['label'].startswith('PLC')],\n",
    "            'Operations': [n for n in self.G.nodes() if self.G.nodes[n]['label'].startswith('HMI')],\n",
    "            'Field': [n for n in self.G.nodes() if 'Device' in self.G.nodes[n]['label'] or \n",
    "                     any(x in self.G.nodes[n]['label'] for x in ['Sensor', 'Actuator'])]\n",
    "        }\n",
    "        \n",
    "        for node in zone_nodes.get(zone, []):\n",
    "            neighbors = list(self.G.neighbors(node))\n",
    "            for neighbor in neighbors:\n",
    "                if not any(neighbor in nodes for nodes in zone_nodes.values()):\n",
    "                    violations.append((node, neighbor))\n",
    "                    \n",
    "        return violations\n",
    "\n",
    "    def _find_exposed_paths(self, asset):\n",
    "        \"\"\"Find potentially exposed paths to critical assets\"\"\"\n",
    "        target_node = None\n",
    "        for node in self.G.nodes():\n",
    "            if self.G.nodes[node]['label'] == asset:\n",
    "                target_node = node\n",
    "                break\n",
    "                \n",
    "        if not target_node:\n",
    "            return []\n",
    "            \n",
    "        paths = []\n",
    "        for node in self.G.nodes():\n",
    "            if node != target_node and nx.has_path(self.G, node, target_node):\n",
    "                path = nx.shortest_path(self.G, node, target_node)\n",
    "                if len(path) <= 3:  # Consider paths of length 3 or less as exposed\n",
    "                    paths.append(path)\n",
    "                    \n",
    "        return paths\n",
    "\n",
    "    def generate_executive_report(self, results):\n",
    "        \"\"\"Generate security-focused report with improved formatting\"\"\"\n",
    "        bottleneck_analysis = results['bottleneck_analysis']\n",
    "        risk_scores = results['risk_scores']\n",
    "        security_metrics = results['security_metrics']\n",
    "        \n",
    "        # Executive Summary section\n",
    "        high_risk_nodes = len(risk_scores[risk_scores['risk_score'] >= 75])\n",
    "        critical_bottlenecks = sum(1 for m in bottleneck_analysis.values() if m['is_critical'])\n",
    "        exposed_assets = sum(1 for m in security_metrics.values() \n",
    "                           if m['min_path_to_plc'] <= 2 or m['min_path_to_hmi'] <= 2)\n",
    "\n",
    "        report = f\"\"\"# ICS/OT Network Security Analysis Report\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "CRITICAL FINDINGS:\n",
    "• {high_risk_nodes} nodes identified as high-risk requiring immediate attention\n",
    "• {critical_bottlenecks} critical network bottlenecks that could impact operations\n",
    "• {exposed_assets} potentially exposed critical assets\n",
    "\n",
    "## Network Vulnerability Assessment\n",
    "\n",
    "### High-Risk Nodes\n",
    "```\n",
    "Node         | Risk Score | PLC Conn | HMI Conn | Field Conn | Exposure\n",
    "-------------|------------|----------|----------|------------|----------\"\"\"\n",
    "\n",
    "        high_risk = risk_scores[risk_scores['risk_score'] >= 75].sort_values('risk_score', ascending=False)\n",
    "        for node in high_risk.index[:5]:\n",
    "            node_label = self.G.nodes[node]['label']\n",
    "            risk_score = risk_scores.loc[node, 'risk_score']\n",
    "            metrics = security_metrics[node]\n",
    "            report += f\"\\n{node_label:<12} | {risk_score:^10.1f} | {metrics['plc_connections']:^8} | {metrics['hmi_connections']:^8} | {metrics['field_connections']:^10} | {risk_scores.loc[node, 'exposure_score']:^8.2f}\"\n",
    "\n",
    "        report += \"\\n```\\n\"\n",
    "\n",
    "        # Bottleneck Analysis section\n",
    "        report += \"\"\"\n",
    "## Network Bottleneck Analysis\n",
    "\n",
    "Critical traffic concentration points ranked by impact:\n",
    "\n",
    "```\n",
    "Node         | Impact Score | Traffic Load | Components | Risk Level\n",
    "-------------|-------------|--------------|------------|------------\"\"\"\n",
    "\n",
    "        critical_points = sorted(\n",
    "            ((node, metrics) for node, metrics in bottleneck_analysis.items() if metrics['is_critical']),\n",
    "            key=lambda x: x[1]['spectral_score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for node, metrics in critical_points[:5]:\n",
    "            impact = metrics['spectral_score']\n",
    "            risk_level = \"HIGH\" if impact > 0.7 else \"MODERATE\"\n",
    "            report += f\"\\n{node:<12} | {impact:^11.3f} | {metrics['flow_centrality']:^11.3f} | {metrics['degree']:^10} | {risk_level:<10}\"\n",
    "\n",
    "        report += \"\\n```\"\n",
    "\n",
    "        # Zone Analysis section\n",
    "        report += \"\"\"\n",
    "\n",
    "## Security Zone Analysis\n",
    "\n",
    "Zone isolation status:\n",
    "\n",
    "```\n",
    "Zone         | Violations | Risk Level | Recommended Action\n",
    "-------------|------------|------------|-------------------\"\"\"\n",
    "        \n",
    "        for zone in ['Control', 'Operations', 'Field']:\n",
    "            violations = self._analyze_zone_isolation(zone, security_metrics)\n",
    "            risk_level = \"HIGH\" if len(violations) > 3 else \"MODERATE\"\n",
    "            action = \"IMMEDIATE REVIEW\" if len(violations) > 3 else \"Monitor and Review\"\n",
    "            report += f\"\\n{zone:<12} | {len(violations):^10} | {risk_level:<10} | {action}\"\n",
    "\n",
    "        report += \"\\n```\"\n",
    "\n",
    "        # Attack Path Analysis section\n",
    "        report += \"\"\"\n",
    "\n",
    "## Attack Path Analysis\n",
    "\n",
    "Critical asset exposure analysis:\n",
    "\n",
    "```\n",
    "Asset        | Attack Paths | Min Path Length | Risk Level\n",
    "-------------|-------------|-----------------|------------\"\"\"\n",
    "        \n",
    "        critical_assets = ['Crown Jewel', 'SCADA Server', 'HMI 1', 'HMI 2', 'HMI 3']\n",
    "        for asset in critical_assets:\n",
    "            paths = self._find_exposed_paths(asset)\n",
    "            if paths:\n",
    "                risk_level = \"HIGH\" if len(paths) > 5 else \"MODERATE\"\n",
    "                report += f\"\\n{asset:<12} | {len(paths):^12} | {min(len(p) for p in paths):^15} | {risk_level:<10}\"\n",
    "\n",
    "        report += \"\\n```\"\n",
    "\n",
    "        # Technical Details section\n",
    "        report += \"\"\"\n",
    "\n",
    "## Appendix: Technical Details\n",
    "\n",
    "```\n",
    "Metric               | Value\n",
    "--------------------|-------\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "Total Nodes         | {self.G.number_of_nodes()}\n",
    "Network Diameter    | {nx.diameter(self.G)}\n",
    "Average Path Length | {nx.average_shortest_path_length(self.G):.2f}\n",
    "Graph Density      | {nx.density(self.G):.3f}\n",
    "Spectral Radius    | {self.calculate_spectral_metrics()['spectral_radius']:.3f}\n",
    "Fiedler Value      | {self.calculate_spectral_metrics()['fiedler_value']:.3f}\n",
    "```\n",
    "Top Fiedler Components:\n",
    "\"\"\"\n",
    "        \n",
    "        for node, value in self.calculate_spectral_metrics()['fiedler_components'][:5]:\n",
    "            report += f\"{node:<12} | {value:.3f}\\n\"\n",
    "\n",
    "        report += \"\\n```\"\n",
    "\n",
    "        return report\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Run complete network analysis\"\"\"\n",
    "        # Basic analysis\n",
    "        structure_analysis = self.analyze_network_structure()\n",
    "        criticality_scores = self.calculate_node_criticality()\n",
    "        security_metrics = self.calculate_security_metrics()\n",
    "        risk_scores = self.detect_anomalies(security_metrics)\n",
    "        spectral_metrics = self.calculate_spectral_metrics()\n",
    "        \n",
    "        # Bottleneck analysis\n",
    "        bottleneck_analysis = self.analyze_bottlenecks()\n",
    "        \n",
    "        results = {\n",
    "            'structure_analysis': structure_analysis,\n",
    "            'criticality_scores': criticality_scores,\n",
    "            'security_metrics': security_metrics,\n",
    "            'risk_scores': risk_scores,\n",
    "            'spectral_metrics': spectral_metrics,\n",
    "            'bottleneck_analysis': bottleneck_analysis\n",
    "        }\n",
    "        \n",
    "        # Generate visualizations and reports\n",
    "        self.visualize_risk_levels(risk_scores)\n",
    "        self.visualize_bottlenecks(bottleneck_analysis)\n",
    "        report = self.generate_executive_report(results)\n",
    "        \n",
    "        # Save the report\n",
    "        report_path = os.path.join(self.output_path, 'security_analysis_report.md')\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "            \n",
    "        print(f\"\\nAnalysis complete. Security report saved to: {report_path}\")\n",
    "        return results\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    base_path = r\"C:\\Users\\Service Casket\\Desktop\\Network Analysis Tool\"\n",
    "    tool = NetworkAnalysisTool(base_path)\n",
    "    results = tool.run_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c7829-a165-4cac-864f-6692fb45501c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
