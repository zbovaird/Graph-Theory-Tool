{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbcc01a-133f-4a2c-b61f-fa4fa5726bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting network analysis...\n",
      "Generating network visualizations...\n",
      "Generating comprehensive analysis report...\n",
      "\n",
      "Comprehensive report saved to: C:\\Users\\Service Casket\\Desktop\\Network Analysis Tool\\output\\comprehensive_analysis_report.md\n",
      "\n",
      "Analysis complete. All reports and visualizations have been saved.\n"
     ]
    }
   ],
   "source": [
    "# Required packages:\n",
    "# pip install networkx numpy pandas matplotlib scikit-learn seaborn pyvis python-louvain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "import community.community_louvain as community_louvain\n",
    "from collections import Counter\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NetworkAnalysisTool:\n",
    "    def __init__(self, base_path):\n",
    "        \"\"\"Initialize the Network Analysis Tool\"\"\"\n",
    "        self.base_path = os.path.expanduser(base_path)\n",
    "        self.output_path = os.path.join(self.base_path, 'output')\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "        \n",
    "        # Set up logging\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(self.output_path, 'analysis.log'),\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Initialize graph\n",
    "        self.create_graph()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load nodes and edges data\"\"\"\n",
    "        try:\n",
    "            self.nodes_df = pd.read_csv(os.path.join(self.base_path, 'ICS_OT Nodes.csv'))\n",
    "            self.edges_df = pd.read_csv(os.path.join(self.base_path, 'ICS_OT Edges.csv'))\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Error loading CSV files: {e}\")\n",
    "            \n",
    "    def create_graph(self):\n",
    "        \"\"\"Create the network graph\"\"\"\n",
    "        self.G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with labels and types\n",
    "        for _, row in self.nodes_df.iterrows():\n",
    "            self.G.add_node(row['Id'], label=row['Label'], shape=row.get('Shape', 'ellipse'))\n",
    "            \n",
    "        # Add edges\n",
    "        for _, row in self.edges_df.iterrows():\n",
    "            self.G.add_edge(row['Source'], row['Target'])\n",
    "            \n",
    "    def analyze_network_structure(self):\n",
    "        \"\"\"Analyze basic network structure\"\"\"\n",
    "        logging.info(\"Analyzing network structure\")\n",
    "        \n",
    "        components = list(nx.connected_components(self.G))\n",
    "        cycles = list(nx.cycle_basis(self.G))\n",
    "        endpoints = [node for node, degree in dict(self.G.degree()).items() if degree == 1]\n",
    "        \n",
    "        # Calculate centrality measures\n",
    "        self.centrality_measures = {\n",
    "            'Degree_Centrality': nx.degree_centrality(self.G),\n",
    "            'Betweenness_Centrality': nx.betweenness_centrality(self.G),\n",
    "            'Closeness_Centrality': nx.closeness_centrality(self.G),\n",
    "            'Eigenvector_Centrality': nx.eigenvector_centrality(self.G, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'components': components,\n",
    "            'cycles': cycles,\n",
    "            'endpoints': endpoints,\n",
    "            'density': nx.density(self.G),\n",
    "            'is_tree': nx.is_tree(self.G),\n",
    "            'is_forest': nx.is_forest(self.G)\n",
    "        }\n",
    "        \n",
    "    def calculate_node_criticality(self):\n",
    "        \"\"\"Calculate node criticality based on network segmentation\"\"\"\n",
    "        original_components = nx.number_connected_components(self.G)\n",
    "        criticality = {}\n",
    "        \n",
    "        for node in self.G.nodes():\n",
    "            G_temp = self.G.copy()\n",
    "            G_temp.remove_node(node)\n",
    "            new_components = nx.number_connected_components(G_temp)\n",
    "            criticality[node] = new_components - original_components\n",
    "            \n",
    "        return criticality\n",
    "        \n",
    "    def calculate_security_metrics(self):\n",
    "        \"\"\"Calculate ICS/SCADA-specific security metrics\"\"\"\n",
    "        control_systems = ['PLC 1', 'PLC 2', 'PLC 3', 'PLC 4', 'PLC 5']\n",
    "        hmi_systems = ['HMI 1', 'HMI 2', 'HMI 3']\n",
    "        field_devices = ['Sensor 1', 'Actuator 1', 'Device 1', 'Device 2', 'Device 3',\n",
    "                        'Device 4', 'Device 5', 'Device 6', 'Device 7', 'Device 8',\n",
    "                        'Device 9', 'Device 10', 'Device 11']\n",
    "                        \n",
    "        plc_nodes = [node for node, attr in self.G.nodes(data=True) if attr['label'] in control_systems]\n",
    "        hmi_nodes = [node for node, attr in self.G.nodes(data=True) if attr['label'] in hmi_systems]\n",
    "        field_nodes = [node for node, attr in self.G.nodes(data=True) if attr['label'] in field_devices]\n",
    "        \n",
    "        metrics = {}\n",
    "        for node in self.G.nodes():\n",
    "            metrics[node] = self._calculate_node_security_metrics(\n",
    "                node, plc_nodes, hmi_nodes, field_nodes,\n",
    "                control_systems, hmi_systems, field_devices\n",
    "            )\n",
    "            \n",
    "        return metrics\n",
    "        \n",
    "    def _calculate_node_security_metrics(self, node, plc_nodes, hmi_nodes, field_nodes,\n",
    "                                       control_systems, hmi_systems, field_devices):\n",
    "        \"\"\"Calculate security metrics for a single node\"\"\"\n",
    "        node_label = self.G.nodes[node]['label']\n",
    "        \n",
    "        min_path_to_plc = float('inf')\n",
    "        min_path_to_hmi = float('inf')\n",
    "        \n",
    "        for plc in plc_nodes:\n",
    "            if nx.has_path(self.G, node, plc):\n",
    "                min_path_to_plc = min(min_path_to_plc, nx.shortest_path_length(self.G, node, plc))\n",
    "                \n",
    "        for hmi in hmi_nodes:\n",
    "            if nx.has_path(self.G, node, hmi):\n",
    "                min_path_to_hmi = min(min_path_to_hmi, nx.shortest_path_length(self.G, node, hmi))\n",
    "                \n",
    "        return {\n",
    "            'plc_connections': sum(1 for n in self.G.neighbors(node) if n in plc_nodes),\n",
    "            'hmi_connections': sum(1 for n in self.G.neighbors(node) if n in hmi_nodes),\n",
    "            'field_connections': sum(1 for n in self.G.neighbors(node) if n in field_nodes),\n",
    "            'min_path_to_plc': min_path_to_plc,\n",
    "            'min_path_to_hmi': min_path_to_hmi,\n",
    "            'is_control_system': int(node_label in control_systems),\n",
    "            'is_hmi': int(node_label in hmi_systems),\n",
    "            'is_field_device': int(node_label in field_devices)\n",
    "        }\n",
    "        \n",
    "    def detect_anomalies(self, security_metrics):\n",
    "        \"\"\"Enhanced anomaly detection with ICS-specific features\"\"\"\n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # Add centrality measures\n",
    "        for measure, values in self.centrality_measures.items():\n",
    "            features[measure] = pd.Series(values)\n",
    "            \n",
    "        # Add security metrics\n",
    "        for node in self.G.nodes():\n",
    "            for metric, value in security_metrics[node].items():\n",
    "                if metric not in features:\n",
    "                    features[metric] = 0\n",
    "                features.loc[node, metric] = float(value if value != float('inf') else 1000)\n",
    "                \n",
    "        # Calculate exposure scores\n",
    "        features['plc_path_factor'] = features['min_path_to_plc'].apply(\n",
    "            lambda x: 1/(x+1) if x < 1000 else 0)\n",
    "        features['hmi_path_factor'] = features['min_path_to_hmi'].apply(\n",
    "            lambda x: 1/(x+1) if x < 1000 else 0)\n",
    "            \n",
    "        features['exposure_score'] = (\n",
    "            features['plc_connections'] * 3 +\n",
    "            features['hmi_connections'] * 2 +\n",
    "            features['field_connections'] +\n",
    "            features['plc_path_factor'] * 2 +\n",
    "            features['hmi_path_factor'] * 1.5\n",
    "        )\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        iso_forest = IsolationForest(contamination=0.1, n_estimators=100, random_state=42)\n",
    "        features['anomaly'] = iso_forest.fit_predict(features_scaled)\n",
    "        features['anomaly_score'] = iso_forest.score_samples(features_scaled)\n",
    "        \n",
    "        # Convert to risk score (0-100)\n",
    "        min_score = features['anomaly_score'].min()\n",
    "        max_score = features['anomaly_score'].max()\n",
    "        features['risk_score'] = 100 * (features['anomaly_score'] - max_score) / (min_score - max_score)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def visualize_risk_levels(self, risk_scores):\n",
    "        \"\"\"Create network visualization with risk levels\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        pos = nx.spring_layout(self.G, k=2, iterations=50)\n",
    "        \n",
    "        # Create node colors and sizes based on risk scores\n",
    "        risk_colors = []\n",
    "        node_sizes = []\n",
    "        for node in self.G.nodes():\n",
    "            risk_score = risk_scores.loc[node, 'risk_score']\n",
    "            \n",
    "            if risk_score >= 75:\n",
    "                color = 'red'\n",
    "                size = 3000\n",
    "            elif risk_score >= 50:\n",
    "                color = 'orange'\n",
    "                size = 2500\n",
    "            elif risk_score >= 25:\n",
    "                color = 'yellow'\n",
    "                size = 2000\n",
    "            else:\n",
    "                color = 'green'\n",
    "                size = 1500\n",
    "                \n",
    "            risk_colors.append(color)\n",
    "            node_sizes.append(size)\n",
    "            \n",
    "        # Draw network\n",
    "        nx.draw_networkx_edges(self.G, pos, edge_color='gray', alpha=0.5)\n",
    "        nodes = nx.draw_networkx_nodes(self.G, pos,\n",
    "                                     node_color=risk_colors,\n",
    "                                     node_size=node_sizes)\n",
    "                                     \n",
    "        # Add labels\n",
    "        labels = nx.get_node_attributes(self.G, 'label')\n",
    "        nx.draw_networkx_labels(self.G, pos, labels, font_size=8)\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                    label=f'{level} Risk',\n",
    "                                    markerfacecolor=color, markersize=10)\n",
    "                          for level, color in [('Critical', 'red'),\n",
    "                                             ('High', 'orange'),\n",
    "                                             ('Moderate', 'yellow'),\n",
    "                                             ('Low', 'green')]]\n",
    "                                             \n",
    "        plt.legend(handles=legend_elements, loc='upper left',\n",
    "                  title='Risk Levels', bbox_to_anchor=(1, 1))\n",
    "                  \n",
    "        plt.title('Network Risk Level Visualization')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        plt.savefig(os.path.join(self.output_path, 'risk_level_visualization.png'),\n",
    "                    bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def generate_comprehensive_report(self, results):\n",
    "        \"\"\"Generate a comprehensive network analysis report in markdown format\"\"\"\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        structure_analysis = results['structure_analysis']\n",
    "        criticality_scores = results['criticality_scores']\n",
    "        security_metrics = results['security_metrics']\n",
    "        risk_scores = results['risk_scores']\n",
    "        \n",
    "        report = f\"\"\"# ICS/OT Network Analysis Report\n",
    "\n",
    "## Network Statistics\n",
    "- Total Nodes: {self.G.number_of_nodes()}\n",
    "- Total Edges: {self.G.number_of_edges()}\n",
    "- Network Density: {structure_analysis['density']:.3f}\n",
    "- Network Diameter: {nx.diameter(self.G)}\n",
    "- Is Tree: {structure_analysis['is_tree']}\n",
    "- Is Forest: {structure_analysis['is_forest']}\n",
    "- Number of Endpoints: {len(structure_analysis['endpoints'])}\n",
    "\n",
    "## Network Structure\n",
    "### Components\n",
    "- Number of Connected Components: {len(structure_analysis['components'])}\n",
    "- Largest Component Size: {max(len(comp) for comp in structure_analysis['components'])}\n",
    "\n",
    "### Cycles\n",
    "- Number of Cycles: {len(structure_analysis['cycles'])}\n",
    "\"\"\"\n",
    "        # Add cycle details\n",
    "        for i, cycle in enumerate(structure_analysis['cycles'], 1):\n",
    "            cycle_nodes = [self.G.nodes[n]['label'] for n in cycle]\n",
    "            report += f\"- Cycle {i}: {' -> '.join(cycle_nodes)} -> {cycle_nodes[0]}\\n\"\n",
    "\n",
    "        report += \"\\n## Critical Nodes Analysis\\n\"\n",
    "        # Sort nodes by criticality\n",
    "        sorted_critical = dict(sorted(criticality_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        for node, score in sorted_critical.items():\n",
    "            node_label = self.G.nodes[node]['label']\n",
    "            report += f\"- {node_label}:\\n\"\n",
    "            report += f\"  * Splits network into {score + 1} components\\n\"\n",
    "            report += f\"  * Degree: {self.G.degree(node)}\\n\"\n",
    "            report += f\"  * Betweenness Centrality: {self.centrality_measures['Betweenness_Centrality'][node]:.3f}\\n\"\n",
    "\n",
    "        report += \"\\n## Security Risk Assessment\\n\"\n",
    "        # Sort nodes by risk score\n",
    "        high_risk_nodes = risk_scores[risk_scores['risk_score'] >= 50].sort_values('risk_score', ascending=False)\n",
    "        \n",
    "        report += \"### High Risk Nodes (Risk Score >= 50)\\n\"\n",
    "        for node in high_risk_nodes.index:\n",
    "            node_label = self.G.nodes[node]['label']\n",
    "            metrics = security_metrics[node]\n",
    "            risk_score = risk_scores.loc[node, 'risk_score']\n",
    "            \n",
    "            report += f\"\\n#### {node_label}\\n\"\n",
    "            report += f\"Risk Score: {risk_score:.1f}/100\\n\"\n",
    "            report += \"Security Metrics:\\n\"\n",
    "            report += f\"- PLC Connections: {metrics['plc_connections']}\\n\"\n",
    "            report += f\"- HMI Connections: {metrics['hmi_connections']}\\n\"\n",
    "            report += f\"- Field Device Connections: {metrics['field_connections']}\\n\"\n",
    "            report += f\"- Minimum Path to PLC: {metrics['min_path_to_plc'] if metrics['min_path_to_plc'] != float('inf') else 'N/A'}\\n\"\n",
    "            report += f\"- Minimum Path to HMI: {metrics['min_path_to_hmi'] if metrics['min_path_to_hmi'] != float('inf') else 'N/A'}\\n\"\n",
    "            report += f\"- Exposure Score: {risk_scores.loc[node, 'exposure_score']:.2f}\\n\"\n",
    "\n",
    "        report += \"\\n## Detected Anomalies\\n\"\n",
    "        # Get nodes with anomalies\n",
    "        anomalous_nodes = risk_scores[risk_scores['anomaly'] == -1].sort_values('risk_score', ascending=False)\n",
    "        \n",
    "        report += \"### Centrality Measures for Anomalous Nodes\\n\"\n",
    "        centrality_cols = ['Degree_Centrality', 'Betweenness_Centrality', \n",
    "                          'Closeness_Centrality', 'Eigenvector_Centrality']\n",
    "        \n",
    "        report += \"```\\n\"  # Start of formatted table\n",
    "        # Add header\n",
    "        report += \"Node Label\".ljust(20)\n",
    "        for col in centrality_cols:\n",
    "            report += col.replace('_', ' ').ljust(25)\n",
    "        report += \"\\n\" + \"-\" * 120 + \"\\n\"\n",
    "        \n",
    "        # Add data rows\n",
    "        for node in anomalous_nodes.index:\n",
    "            node_label = self.G.nodes[node]['label']\n",
    "            report += f\"{node_label[:19].ljust(20)}\"\n",
    "            for col in centrality_cols:\n",
    "                value = self.centrality_measures[col][node]\n",
    "                report += f\"{value:.6f}\".ljust(25)\n",
    "            report += \"\\n\"\n",
    "        report += \"```\\n\"  # End of formatted table\n",
    "\n",
    "        report += \"\\n## Path Analysis\\n\"\n",
    "        # Add path analysis for critical assets\n",
    "        critical_assets = ['Crown Jewel', 'SCADA Server', 'HMI 1', 'HMI 2', 'HMI 3']\n",
    "        critical_nodes = [node for node, attr in self.G.nodes(data=True) \n",
    "                         if attr['label'] in critical_assets]\n",
    "        \n",
    "        report += \"### Shortest Paths to Critical Assets\\n\"\n",
    "        for start_node in self.G.nodes():\n",
    "            start_label = self.G.nodes[start_node]['label']\n",
    "            report += f\"\\nFrom {start_label}:\\n\"\n",
    "            for end_node in critical_nodes:\n",
    "                end_label = self.G.nodes[end_node]['label']\n",
    "                if nx.has_path(self.G, start_node, end_node):\n",
    "                    path = nx.shortest_path(self.G, start_node, end_node)\n",
    "                    path_labels = [self.G.nodes[n]['label'] for n in path]\n",
    "                    report += f\"- To {end_label}: {' -> '.join(path_labels)} ({len(path)-1} hops)\\n\"\n",
    "                else:\n",
    "                    report += f\"- To {end_label}: No path exists\\n\"\n",
    "\n",
    "        report += \"\\n## Analysis Details\\n\"\n",
    "        report += f\"- Analysis completed at: {timestamp}\\n\"\n",
    "        report += f\"- Output directory: {self.output_path}\\n\"\n",
    "        report += \"\\n## Generated Files\\n\"\n",
    "        report += \"- risk_level_visualization.png\\n\"\n",
    "        report += \"- network_analysis_report.txt\\n\"\n",
    "        report += \"- security_analysis_report.txt\\n\"\n",
    "        report += \"- comprehensive_analysis_report.md\\n\"\n",
    "\n",
    "        # Add summary statistics\n",
    "        report += \"\\n## Summary Statistics\\n\"\n",
    "        report += f\"- Average node degree: {round(np.mean([d for n, d in self.G.degree()]), 2)}\\n\"\n",
    "        report += f\"- Graph diameter: {nx.diameter(self.G)}\\n\"\n",
    "        report += f\"- Average shortest path length: {round(nx.average_shortest_path_length(self.G), 2)}\\n\"\n",
    "        report += f\"- Graph density: {round(nx.density(self.G), 3)}\\n\"\n",
    "        report += f\"- Number of high-risk nodes: {len(high_risk_nodes)}\\n\"\n",
    "        report += f\"- Number of anomalous nodes: {len(anomalous_nodes)}\\n\"\n",
    "        \n",
    "        # Save the report\n",
    "        report_path = os.path.join(self.output_path, 'comprehensive_analysis_report.md')\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "            \n",
    "        print(f\"\\nComprehensive report saved to: {report_path}\")\n",
    "        return report\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Run complete network analysis\"\"\"\n",
    "        print(\"Starting network analysis...\")\n",
    "        \n",
    "        # Basic network structure analysis\n",
    "        structure_analysis = self.analyze_network_structure()\n",
    "        \n",
    "        # Calculate node criticality\n",
    "        criticality_scores = self.calculate_node_criticality()\n",
    "        \n",
    "        # Security analysis\n",
    "        security_metrics = self.calculate_security_metrics()\n",
    "        risk_scores = self.detect_anomalies(security_metrics)\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'structure_analysis': structure_analysis,\n",
    "            'criticality_scores': criticality_scores,\n",
    "            'security_metrics': security_metrics,\n",
    "            'risk_scores': risk_scores\n",
    "        }\n",
    "        \n",
    "        # Generate visualizations\n",
    "        print(\"Generating network visualizations...\")\n",
    "        self.visualize_risk_levels(risk_scores)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        print(\"Generating comprehensive analysis report...\")\n",
    "        self.generate_comprehensive_report(results)\n",
    "        \n",
    "        print(\"\\nAnalysis complete. All reports and visualizations have been saved.\")\n",
    "        return results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize and run the analysis\n",
    "        base_path = os.path.join(os.path.expanduser('~'), 'Desktop', 'Network Analysis Tool')\n",
    "        analyzer = NetworkAnalysisTool(base_path)\n",
    "        results = analyzer.run_analysis()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during analysis: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cbd474-1b89-477d-95b2-b5f70588b147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
