{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc2e08f-a8e5-4357-826b-908e3b78acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating incident response analysis...\n",
      "\n",
      "==================================================\n",
      "INCIDENT RESPONSE SUMMARY DASHBOARD\n",
      "==================================================\n",
      "\n",
      "PRIORITY ACTION ITEMS:\n",
      "+------------+----------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "| Priority   | Category             | Finding                                                             | Action                                                                                |\n",
      "+============+======================+=====================================================================+=======================================================================================+\n",
      "| HIGH       | Network Segmentation | Critical node 23 could split network into 4 segments if compromised | Implement redundant paths around node 23; Consider network segmentation at this point |\n",
      "+------------+----------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "| HIGH       | Network Segmentation | Critical node 5 could split network into 3 segments if compromised  | Implement redundant paths around node 5; Consider network segmentation at this point  |\n",
      "+------------+----------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "| HIGH       | Network Segmentation | Critical node 14 could split network into 3 segments if compromised | Implement redundant paths around node 14; Consider network segmentation at this point |\n",
      "+------------+----------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "| HIGH       | Attack Vector        | Critical path identified: 2 -> 5 -> 12                              | Implement access controls and monitoring along this path                              |\n",
      "+------------+----------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "| HIGH       | Attack Vector        | Critical path identified: 5 -> 2 -> 6                               | Implement access controls and monitoring along this path                              |\n",
      "+------------+----------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "| HIGH       | Attack Vector        | Critical path identified: 6 -> 2 -> 5                               | Implement access controls and monitoring along this path                              |\n",
      "+------------+----------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "\n",
      "KEY METRICS:\n",
      "+---------------------+---------+\n",
      "| Metric              |   Value |\n",
      "+=====================+=========+\n",
      "| New Connections     |       3 |\n",
      "+---------------------+---------+\n",
      "| Removed Connections |       0 |\n",
      "+---------------------+---------+\n",
      "| Affected Nodes      |       0 |\n",
      "+---------------------+---------+\n",
      "| Network Segments    |       6 |\n",
      "+---------------------+---------+\n",
      "| Anomalous Nodes     |       3 |\n",
      "+---------------------+---------+\n",
      "\n",
      "RECOMMENDED IMMEDIATE ACTIONS:\n",
      "1. Review and action all HIGH priority items above\n",
      "2. Monitor anomalous nodes for suspicious activity\n",
      "3. Review new network connections for unauthorized changes\n",
      "4. Implement segmentation at identified bridge nodes\n",
      "\n",
      "Detailed analysis and visualizations saved to output folder.\n",
      "\n",
      "Saving detailed analysis to output folder...\n",
      "\n",
      "Analysis complete. Results saved to: C:\\Users\\Service Casket\\Desktop\\Network Analysis Tool\\post-incident\\output\n",
      "\n",
      "Recommended next steps:\n",
      "1. Review the action_items.md file for prioritized response actions\n",
      "2. Examine critical_infrastructure.png to understand vulnerable network points\n",
      "3. Check node_metrics_changes_heatmap.png for detailed node behavior changes\n",
      "4. Review the README.md for a complete analysis summary\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, List\n",
    "import community  # python-louvain package\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Suppress specific FutureWarnings from Seaborn\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='seaborn')\n",
    "\n",
    "def load_graph(nodes_file: str, edges_file: str) -> Tuple[nx.Graph, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load network data from CSV files and create a NetworkX graph.\n",
    "\n",
    "    Parameters:\n",
    "    - nodes_file: Path to the nodes CSV file.\n",
    "    - edges_file: Path to the edges CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - G: NetworkX graph.\n",
    "    - nodes_df: DataFrame containing node attributes.\n",
    "    - edges_df: DataFrame containing edge attributes.\n",
    "    \"\"\"\n",
    "    nodes_df = pd.read_csv(nodes_file)\n",
    "    edges_df = pd.read_csv(edges_file)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add node attributes as a dictionary\n",
    "    node_attrs = nodes_df.set_index('Id').to_dict('index')\n",
    "    G.add_nodes_from([(node, attrs) for node, attrs in node_attrs.items()])\n",
    "    \n",
    "    # Add edges with weights if available\n",
    "    if 'Weight' in edges_df.columns:\n",
    "        G.add_weighted_edges_from([(row['Source'], row['Target'], row['Weight']) \n",
    "                                 for _, row in edges_df.iterrows()])\n",
    "    else:\n",
    "        G.add_edges_from([(row['Source'], row['Target']) \n",
    "                         for _, row in edges_df.iterrows()])\n",
    "    \n",
    "    return G, nodes_df, edges_df\n",
    "\n",
    "def calculate_network_metrics(G: nx.Graph) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive network metrics, including spectral radius and Fiedler eigenvalue.\n",
    "\n",
    "    Parameters:\n",
    "    - G: NetworkX graph.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing network metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'Average Clustering': nx.average_clustering(G),\n",
    "        'Network Density': nx.density(G),\n",
    "        'Average Degree': sum(dict(G.degree()).values()) / G.number_of_nodes(),\n",
    "        'Components': nx.number_connected_components(G)\n",
    "    }\n",
    "    \n",
    "    # Check if the graph is connected\n",
    "    if nx.is_connected(G):\n",
    "        # Average Path Length and Diameter\n",
    "        metrics['Average Path Length'] = nx.average_shortest_path_length(G)\n",
    "        metrics['Network Diameter'] = nx.diameter(G)\n",
    "        \n",
    "        # Spectral Radius\n",
    "        A = nx.adjacency_matrix(G).todense()\n",
    "        eigvals = np.linalg.eigvals(A)\n",
    "        spectral_radius = np.max(np.abs(eigvals))\n",
    "        metrics['Spectral Radius'] = spectral_radius\n",
    "        \n",
    "        # Fiedler Eigenvalue\n",
    "        L = nx.laplacian_matrix(G).todense()\n",
    "        eigvals_L = np.linalg.eigvalsh(L)  # Since Laplacian is symmetric\n",
    "        if len(eigvals_L) >= 2:\n",
    "            fiedler_eigval = eigvals_L[1]\n",
    "            metrics['Fiedler Eigenvalue'] = fiedler_eigval\n",
    "        else:\n",
    "            metrics['Fiedler Eigenvalue'] = np.nan\n",
    "    else:\n",
    "        metrics['Average Path Length'] = np.nan\n",
    "        metrics['Network Diameter'] = np.nan\n",
    "        metrics['Spectral Radius'] = np.nan\n",
    "        metrics['Fiedler Eigenvalue'] = np.nan\n",
    "    \n",
    "    try:\n",
    "        communities = community.best_partition(G)\n",
    "        metrics['Number of Communities'] = len(set(communities.values()))\n",
    "    except Exception as e:\n",
    "        print(f\"Community detection failed: {e}\")\n",
    "        metrics['Number of Communities'] = np.nan\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_node_metrics(G: nx.Graph) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive node-level metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - G: NetworkX graph.\n",
    "\n",
    "    Returns:\n",
    "    - node_metrics_df: DataFrame containing node-level metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'Degree Centrality': nx.degree_centrality(G),\n",
    "        'Betweenness Centrality': nx.betweenness_centrality(G),\n",
    "        'Closeness Centrality': nx.closeness_centrality(G),\n",
    "        'Eigenvector Centrality': nx.eigenvector_centrality(G, max_iter=1000),\n",
    "        'Clustering Coefficient': nx.clustering(G),\n",
    "        'PageRank': nx.pagerank(G)\n",
    "    }\n",
    "    node_metrics_df = pd.DataFrame(metrics).sort_index()\n",
    "    \n",
    "    # Compute Fiedler Vector if the graph is connected\n",
    "    if nx.is_connected(G):\n",
    "        L = nx.laplacian_matrix(G).todense()\n",
    "        eigvals_L, eigvecs_L = np.linalg.eig(L)\n",
    "        idx = eigvals_L.argsort()\n",
    "        eigvals_L = eigvals_L[idx]\n",
    "        eigvecs_L = eigvecs_L[:, idx]\n",
    "        \n",
    "        if len(eigvals_L) >= 2:\n",
    "            fiedler_vector = eigvecs_L[:, 1]\n",
    "            node_metrics_df['Fiedler Vector'] = fiedler_vector\n",
    "        else:\n",
    "            node_metrics_df['Fiedler Vector'] = np.nan\n",
    "    else:\n",
    "        node_metrics_df['Fiedler Vector'] = np.nan\n",
    "    \n",
    "    return node_metrics_df\n",
    "\n",
    "def analyze_changes(G1: nx.Graph, G2: nx.Graph) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze structural changes between two network snapshots.\n",
    "\n",
    "    Parameters:\n",
    "    - G1: First NetworkX graph.\n",
    "    - G2: Second NetworkX graph.\n",
    "\n",
    "    Returns:\n",
    "    - changes: Dictionary containing structural changes.\n",
    "    \"\"\"\n",
    "    changes = {\n",
    "        'New Nodes': len(set(G2.nodes()) - set(G1.nodes())),\n",
    "        'Removed Nodes': len(set(G1.nodes()) - set(G2.nodes())),\n",
    "        'New Edges': len(set(G2.edges()) - set(G1.edges())),\n",
    "        'Removed Edges': len(set(G1.edges()) - set(G2.edges())),\n",
    "    }\n",
    "    \n",
    "    degrees1 = Counter(dict(G1.degree()).values())\n",
    "    degrees2 = Counter(dict(G2.degree()).values())\n",
    "    changes['Degree Distribution Change'] = sum((degrees2 - degrees1).values())\n",
    "    \n",
    "    return changes\n",
    "\n",
    "def calculate_k_core(G: nx.Graph) -> Dict:\n",
    "    \"\"\"Calculate core numbers for the graph.\"\"\"\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def calculate_edge_betweenness(G: nx.Graph) -> Dict[Tuple, float]:\n",
    "    \"\"\"Calculate edge betweenness centrality.\"\"\"\n",
    "    return nx.edge_betweenness_centrality(G)\n",
    "\n",
    "def calculate_node_strength(G: nx.Graph) -> Dict:\n",
    "    \"\"\"Calculate node strength.\"\"\"\n",
    "    node_strength = {}\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        weight = data.get('Weight', 1)\n",
    "        node_strength[u] = node_strength.get(u, 0) + weight\n",
    "        node_strength[v] = node_strength.get(v, 0) + weight\n",
    "    return node_strength\n",
    "\n",
    "def calculate_assortativity(G: nx.Graph) -> float:\n",
    "    \"\"\"Calculate degree assortativity coefficient.\"\"\"\n",
    "    return nx.degree_assortativity_coefficient(G)\n",
    "\n",
    "def calculate_local_efficiency(G: nx.Graph) -> float:\n",
    "    \"\"\"Calculate average local efficiency.\"\"\"\n",
    "    return nx.local_efficiency(G)\n",
    "\n",
    "def assess_network_robustness(G: nx.Graph, removal_fraction=0.1) -> Dict:\n",
    "    \"\"\"\n",
    "    Assess network robustness by simulating targeted attacks.\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - removal_fraction: Fraction of nodes/edges to remove (default 0.1 or 10%)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing the size of largest connected component after node/edge removal\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    original_largest_component_size = len(max(nx.connected_components(G), key=len))\n",
    "    \n",
    "    # Simulate node removal attack\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G)\n",
    "    nodes_sorted = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
    "    nodes_to_remove = int(removal_fraction * G.number_of_nodes())\n",
    "    nodes_removed = [node for node, degree in nodes_sorted[:nodes_to_remove]]\n",
    "    \n",
    "    G_temp = G.copy()\n",
    "    G_temp.remove_nodes_from(nodes_removed)\n",
    "    if G_temp.number_of_nodes() > 0:\n",
    "        largest_connected_component = max(nx.connected_components(G_temp), key=len)\n",
    "        results['Largest_Connected_Component_After_Node_Removal'] = len(largest_connected_component)\n",
    "        results['Connected_Component_Size_Change_After_Node_Removal'] = (\n",
    "            len(largest_connected_component) - original_largest_component_size\n",
    "        )\n",
    "    else:\n",
    "        results['Largest_Connected_Component_After_Node_Removal'] = 0\n",
    "        results['Connected_Component_Size_Change_After_Node_Removal'] = -original_largest_component_size\n",
    "    \n",
    "    # Simulate edge removal attack\n",
    "    edges_sorted = sorted(edge_betweenness.items(), key=lambda x: x[1], reverse=True)\n",
    "    edges_to_remove = int(removal_fraction * G.number_of_edges())\n",
    "    edges_removed = [edge for edge, centrality in edges_sorted[:edges_to_remove]]\n",
    "    \n",
    "    G_temp = G.copy()\n",
    "    G_temp.remove_edges_from(edges_removed)\n",
    "    if G_temp.number_of_edges() > 0:\n",
    "        largest_connected_component = max(nx.connected_components(G_temp), key=len)\n",
    "        results['Largest_Connected_Component_After_Edge_Removal'] = len(largest_connected_component)\n",
    "        results['Connected_Component_Size_Change_After_Edge_Removal'] = (\n",
    "            len(largest_connected_component) - original_largest_component_size\n",
    "        )\n",
    "    else:\n",
    "        results['Largest_Connected_Component_After_Edge_Removal'] = 0\n",
    "        results['Connected_Component_Size_Change_After_Edge_Removal'] = -original_largest_component_size\n",
    "    \n",
    "    # Add percentage changes\n",
    "    results['Percentage_Of_Network_Intact_After_Node_Removal'] = (\n",
    "        results['Largest_Connected_Component_After_Node_Removal'] / original_largest_component_size * 100\n",
    "    )\n",
    "    results['Percentage_Of_Network_Intact_After_Edge_Removal'] = (\n",
    "        results['Largest_Connected_Component_After_Edge_Removal'] / original_largest_component_size * 100\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def identify_critical_paths(G: nx.Graph) -> Dict:\n",
    "    \"\"\"Identify critical paths between high-centrality nodes that could be attack vectors.\"\"\"\n",
    "    # Get top 10% of nodes by betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    threshold = np.percentile(list(betweenness.values()), 90)\n",
    "    critical_nodes = [n for n, c in betweenness.items() if c >= threshold]\n",
    "    \n",
    "    critical_paths = {}\n",
    "    for source in critical_nodes:\n",
    "        for target in critical_nodes:\n",
    "            if source != target:\n",
    "                try:\n",
    "                    path = nx.shortest_path(G, source, target)\n",
    "                    if len(path) > 2:  # Only paths with intermediate nodes\n",
    "                        critical_paths[f\"{source}->{target}\"] = path\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "    return critical_paths\n",
    "\n",
    "def identify_bridge_nodes(G: nx.Graph) -> List:\n",
    "    \"\"\"Find nodes that, if removed, would significantly fragment the network.\"\"\"\n",
    "    bridges = []\n",
    "    for node in G.nodes():\n",
    "        G_temp = G.copy()\n",
    "        G_temp.remove_node(node)\n",
    "        original_components = nx.number_connected_components(G)\n",
    "        new_components = nx.number_connected_components(G_temp)\n",
    "        if new_components > original_components:\n",
    "            bridges.append((node, new_components - original_components))\n",
    "    return sorted(bridges, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def calculate_network_segmentation(G: nx.Graph) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze network segmentation to identify potential containment boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing segmentation metrics\n",
    "    \"\"\"\n",
    "    # Detect communities using Louvain method\n",
    "    communities = community.best_partition(G)\n",
    "    modularity = community.modularity(communities, G)\n",
    "    \n",
    "    # Count nodes in each segment\n",
    "    segment_sizes = Counter(communities.values())\n",
    "    \n",
    "    # Count edges between segments\n",
    "    cross_segment_edges = sum(1 for u, v in G.edges() \n",
    "                            if communities[u] != communities[v])\n",
    "    \n",
    "    # Calculate isolation score (ratio of internal to external edges)\n",
    "    total_edges = G.number_of_edges()\n",
    "    isolation_score = 1 - (cross_segment_edges / total_edges) if total_edges > 0 else 0\n",
    "    \n",
    "    segment_metrics = {\n",
    "        'num_segments': len(set(communities.values())),\n",
    "        'modularity': modularity,\n",
    "        'segment_sizes': segment_sizes,\n",
    "        'cross_segment_edges': cross_segment_edges,\n",
    "        'isolation_score': isolation_score,\n",
    "        'communities': communities  # Include community assignments for nodes\n",
    "    }\n",
    "    \n",
    "    return segment_metrics\n",
    "\n",
    "def find_potential_lateral_movement(G: nx.Graph) -> List[Tuple]:\n",
    "    \"\"\"Identify paths that could be used for lateral movement.\"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    for component in nx.connected_components(G):\n",
    "        subgraph = G.subgraph(component)\n",
    "        if nx.is_connected(subgraph):\n",
    "            # Find all pairs shortest paths\n",
    "            all_pairs = dict(nx.all_pairs_shortest_path(subgraph))\n",
    "            \n",
    "            # Find the longest shortest path (diameter path)\n",
    "            max_length = 0\n",
    "            longest_path = None\n",
    "            \n",
    "            for source in all_pairs:\n",
    "                for target, path in all_pairs[source].items():\n",
    "                    if len(path) > max_length:\n",
    "                        max_length = len(path)\n",
    "                        longest_path = path\n",
    "            \n",
    "            if longest_path and len(longest_path) >= 3:  # Only consider paths that could represent lateral movement\n",
    "                paths.append(longest_path)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def generate_action_items(G1: nx.Graph, G2: nx.Graph, \n",
    "                         bridge_nodes: List, \n",
    "                         critical_paths: Dict,\n",
    "                         segmentation: Dict,\n",
    "                         anomalies: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate prioritized action items for network administrators.\"\"\"\n",
    "    action_items = []\n",
    "    \n",
    "    # Check for highly impactful bridge nodes\n",
    "    for node, impact in bridge_nodes[:3]:  # Top 3 most critical\n",
    "        action_items.append({\n",
    "            'Priority': 'HIGH',\n",
    "            'Category': 'Network Segmentation',\n",
    "            'Finding': f'Critical node {node} could split network into {impact} segments if compromised',\n",
    "            'Action': f'Implement redundant paths around node {node}; Consider network segmentation at this point'\n",
    "        })\n",
    "    \n",
    "    # Analyze potential attack paths\n",
    "    for path_name, path in list(critical_paths.items())[:3]:\n",
    "        action_items.append({\n",
    "            'Priority': 'HIGH',\n",
    "            'Category': 'Attack Vector',\n",
    "            'Finding': f'Critical path identified: {\" -> \".join(map(str, path))}',\n",
    "            'Action': 'Implement access controls and monitoring along this path'\n",
    "        })\n",
    "    \n",
    "    # Check for anomalous nodes\n",
    "    anomalous_nodes = anomalies[anomalies['anomaly'] == -1].index.tolist()\n",
    "    for node in anomalous_nodes:\n",
    "        action_items.append({\n",
    "            'Priority': 'MEDIUM',\n",
    "            'Category': 'Anomalous Behavior',\n",
    "            'Finding': f'Node {node} shows unusual connectivity patterns',\n",
    "            'Action': 'Investigate traffic patterns and implement enhanced monitoring'\n",
    "        })\n",
    "    \n",
    "    # Assess segmentation issues\n",
    "    if segmentation['cross_segment_edges'] > len(G2.edges()) * 0.3:  # More than 30% cross-segment\n",
    "        action_items.append({\n",
    "            'Priority': 'MEDIUM',\n",
    "            'Category': 'Network Segmentation',\n",
    "            'Finding': 'High number of cross-segment connections detected',\n",
    "            'Action': 'Review and strengthen network segmentation policies'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(action_items)\n",
    "\n",
    "def visualize_network_overview(G1: nx.Graph, G2: nx.Graph, output_folder: str):\n",
    "    \"\"\"Create and display network visualization.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 15))\n",
    "    \n",
    "    degrees1 = [d for n, d in G1.degree()]\n",
    "    degrees2 = [d for n, d in G2.degree()]\n",
    "    \n",
    "    degrees1 = [np.nan if np.isinf(d) else d for d in degrees1]\n",
    "    degrees2 = [np.nan if np.isinf(d) else d for d in degrees2]\n",
    "    \n",
    "    sns.histplot(degrees1, color='blue', label='Before', kde=False, stat=\"density\", bins=20, alpha=0.5, ax=axs[0,0])\n",
    "    sns.histplot(degrees2, color='green', label='After', kde=False, stat=\"density\", bins=20, alpha=0.5, ax=axs[0,0])\n",
    "    axs[0,0].set_title('Degree Distribution Comparison')\n",
    "    axs[0,0].set_xlabel('Degree')\n",
    "    axs[0,0].set_ylabel('Density')\n",
    "    axs[0,0].legend()\n",
    "    \n",
    "    pos_before = nx.spring_layout(G1, seed=42)\n",
    "    nx.draw(G1, pos_before, node_size=50, alpha=0.6, with_labels=False, node_color='blue', ax=axs[0,1])\n",
    "    axs[0,1].set_title('Network Before Incident')\n",
    "    \n",
    "    pos_after = nx.spring_layout(G2, seed=42)\n",
    "    nx.draw(G2, pos_after, node_size=50, alpha=0.6, with_labels=False, node_color='green', ax=axs[1,0])\n",
    "    axs[1,0].set_title('Network After Incident')\n",
    "    \n",
    "    axs[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'network_overview.png'))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_node_metrics_heatmap(centrality_changes: pd.DataFrame, output_folder: str):\n",
    "    \"\"\"Create and display heatmap of changes in node metrics.\"\"\"\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    \n",
    "    sns.heatmap(centrality_changes, \n",
    "                annot=True,\n",
    "                cmap='coolwarm',\n",
    "                center=0,\n",
    "                fmt='.3f',\n",
    "                cbar_kws={'label': 'Change in Centrality'},\n",
    "                square=False,\n",
    "                linewidths=0.5)\n",
    "    \n",
    "    plt.title('Changes in Centrality Measures for All Nodes', pad=20)\n",
    "    plt.xlabel('Centrality Metrics', labelpad=10)\n",
    "    plt.ylabel('Nodes', labelpad=10)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(output_folder, 'node_metrics_changes_heatmap.png'))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_critical_infrastructure(G: nx.Graph, \n",
    "                                    bridge_nodes: List,\n",
    "                                    critical_paths: Dict,\n",
    "                                    output_folder: str):\n",
    "    \"\"\"Create visualization highlighting critical infrastructure.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "    \n",
    "    # Draw base network\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2, edge_color='gray')\n",
    "    \n",
    "    # Draw nodes with size based on importance\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    bridge_nodes_set = {node for node, _ in bridge_nodes}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        if node in bridge_nodes_set:\n",
    "            node_colors.append('red')\n",
    "            node_sizes.append(300)\n",
    "        else:\n",
    "            node_colors.append('lightblue')\n",
    "            node_sizes.append(100)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                          node_size=node_sizes)\n",
    "    \n",
    "    # Highlight critical paths\n",
    "    for path in list(critical_paths.values())[:3]:  # Top 3 critical paths\n",
    "        path_edges = list(zip(path[:-1], path[1:]))\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=path_edges, \n",
    "                             edge_color='red', width=2)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(color='red', label='Bridge Nodes'),\n",
    "        mpatches.Patch(color='lightblue', label='Regular Nodes'),\n",
    "        mpatches.Patch(color='red', alpha=0.5, label='Critical Paths')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    plt.title(\"Critical Infrastructure Analysis\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'critical_infrastructure.png'))\n",
    "    plt.close()\n",
    "\n",
    "def display_summary_dashboard(action_items: pd.DataFrame, \n",
    "                            changes: Dict,\n",
    "                            segmentation: Dict,\n",
    "                            anomalies: pd.DataFrame):\n",
    "    \"\"\"Display a concise summary dashboard with key findings and metrics.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INCIDENT RESPONSE SUMMARY DASHBOARD\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nPRIORITY ACTION ITEMS:\")\n",
    "    print(tabulate(action_items[action_items['Priority'] == 'HIGH'],\n",
    "                  headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    print(\"\\nKEY METRICS:\")\n",
    "    metrics_table = [\n",
    "        [\"New Connections\", changes.get('New Edges', 0)],\n",
    "        [\"Removed Connections\", changes.get('Removed Edges', 0)],\n",
    "        [\"Affected Nodes\", changes.get('New Nodes', 0) + changes.get('Removed Nodes', 0)],\n",
    "        [\"Network Segments\", segmentation['num_segments']],\n",
    "        [\"Anomalous Nodes\", len(anomalies[anomalies['anomaly'] == -1])],\n",
    "    ]\n",
    "    print(tabulate(metrics_table, headers=['Metric', 'Value'], tablefmt='grid'))\n",
    "    \n",
    "    print(\"\\nRECOMMENDED IMMEDIATE ACTIONS:\")\n",
    "    print(\"1. Review and action all HIGH priority items above\")\n",
    "    print(\"2. Monitor anomalous nodes for suspicious activity\")\n",
    "    print(\"3. Review new network connections for unauthorized changes\")\n",
    "    print(\"4. Implement segmentation at identified bridge nodes\")\n",
    "    print(\"\\nDetailed analysis and visualizations saved to output folder.\")\n",
    "\n",
    "def save_metrics_to_csv(spectral_df: pd.DataFrame, other_changes_df: pd.DataFrame,\n",
    "                       structural_changes_df: pd.DataFrame, output_folder: str):\n",
    "    \"\"\"Save metrics to CSV files.\"\"\"\n",
    "    spectral_df.to_csv(os.path.join(output_folder, 'spectral_metrics_changes.csv'), index=False)\n",
    "    other_changes_df.to_csv(os.path.join(output_folder, 'other_network_changes.csv'), index=False)\n",
    "    structural_changes_df.to_csv(os.path.join(output_folder, 'structural_changes.csv'), index=False)\n",
    "\n",
    "def save_anomalies_to_csv(anomalies: pd.DataFrame, output_folder: str):\n",
    "    \"\"\"Save anomalies to CSV.\"\"\"\n",
    "    anomalies_to_save = anomalies[anomalies['anomaly'] == -1]\n",
    "    anomalies_to_save.to_csv(os.path.join(output_folder, 'node_metrics_changes.csv'), index=False)\n",
    "\n",
    "def save_action_items(action_items: pd.DataFrame, output_folder: str):\n",
    "    \"\"\"Save action items to both CSV and markdown formats.\"\"\"\n",
    "    # Save to CSV\n",
    "    action_items.to_csv(os.path.join(output_folder, 'action_items.csv'), index=False)\n",
    "    \n",
    "    # Save to markdown\n",
    "    with open(os.path.join(output_folder, 'action_items.md'), 'w') as f:\n",
    "        f.write(\"# Incident Response Action Items\\n\\n\")\n",
    "        f.write(\"## Priority Actions\\n\\n\")\n",
    "        f.write(tabulate(action_items[action_items['Priority'] == 'HIGH'], \n",
    "                        headers='keys', tablefmt='pipe', showindex=False))\n",
    "        f.write(\"\\n\\n## Secondary Actions\\n\\n\")\n",
    "        f.write(tabulate(action_items[action_items['Priority'] == 'MEDIUM'], \n",
    "                        headers='keys', tablefmt='pipe', showindex=False))\n",
    "\n",
    "def save_additional_metrics(k_core2: Dict, assortativity1: float, assortativity2: float,\n",
    "                          local_efficiency1: float, local_efficiency2: float,\n",
    "                          robustness1: Dict, robustness2: Dict, output_folder: str):\n",
    "    \"\"\"Save additional metrics to text file.\"\"\"\n",
    "    with open(os.path.join(output_folder, 'additional_metrics.txt'), 'w') as f:\n",
    "        f.write(\"=== K-Core Numbers (After) ===\\n\")\n",
    "        for node, core in k_core2.items():\n",
    "            f.write(f\"{node}: {core}\\n\")\n",
    "        \n",
    "        f.write(\"\\n=== Degree Assortativity Coefficient ===\\n\")\n",
    "        f.write(f\"Before: {assortativity1:.6f}, After: {assortativity2:.6f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n=== Local Efficiency ===\\n\")\n",
    "        f.write(f\"Before: {local_efficiency1:.6f}, After: {local_efficiency2:.6f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n=== Network Robustness ===\\n\")\n",
    "        f.write(\"Before Removal:\\n\")\n",
    "        for key, value in robustness1.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        f.write(\"After Removal:\\n\")\n",
    "        for key, value in robustness2.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "def create_readme(spectral_df: pd.DataFrame, other_changes_df: pd.DataFrame,\n",
    "                 structural_changes_df: pd.DataFrame, anomalies: pd.DataFrame,\n",
    "                 output_folder: str):\n",
    "    \"\"\"Create README.md summarizing metrics.\"\"\"\n",
    "    readme_path = os.path.join(output_folder, 'README.md')\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(\"# Post-Incident Network Analysis Report\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Network Overview\\n\\n\")\n",
    "        f.write(\"![Network Overview](network_overview.png)\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Critical Infrastructure Analysis\\n\\n\")\n",
    "        f.write(\"![Critical Infrastructure](critical_infrastructure.png)\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Spectral Metrics\\n\\n\")\n",
    "        f.write(spectral_df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Other Network-Level Changes\\n\\n\")\n",
    "        f.write(other_changes_df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Structural Changes\\n\\n\")\n",
    "        f.write(structural_changes_df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Node Metrics Changes Heatmap\\n\\n\")\n",
    "        f.write(\"![Node Metrics Changes Heatmap](node_metrics_changes_heatmap.png)\\n\\n\")\n",
    "        \n",
    "        if not anomalies.empty:\n",
    "            f.write(\"### Detected Anomalies\\n\\n\")\n",
    "            f.write(anomalies[anomalies['anomaly'] == -1].to_markdown(index=False))\n",
    "        else:\n",
    "            f.write(\"### No anomalies detected\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Additional Metrics\\n\\n\")\n",
    "        with open(os.path.join(output_folder, 'additional_metrics.txt'), 'r') as metrics_file:\n",
    "            f.write(f\"```plaintext\\n{metrics_file.read()}\\n```\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    input_folder = r'C:\\Users\\Service Casket\\Desktop\\Network Analysis Tool\\post-incident'\n",
    "    nodes_file_before = os.path.join(input_folder, 'ICS_OT Nodes.csv')\n",
    "    edges_file_before = os.path.join(input_folder, 'ICS_OT Edges.csv')\n",
    "    nodes_file_after = os.path.join(input_folder, 'ICS_OT NodesInfected.csv')\n",
    "    edges_file_after = os.path.join(input_folder, 'ICS_OT EdgesInfected.csv')\n",
    "    output_folder = os.path.join(input_folder, 'output')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Load networks\n",
    "    G1, nodes_df1, edges_df1 = load_graph(nodes_file_before, edges_file_before)\n",
    "    G2, nodes_df2, edges_df2 = load_graph(nodes_file_after, edges_file_after)\n",
    "\n",
    "    # Calculate metrics\n",
    "    network_metrics1 = calculate_network_metrics(G1)\n",
    "    network_metrics2 = calculate_network_metrics(G2)\n",
    "    node_metrics1 = calculate_node_metrics(G1)\n",
    "    node_metrics2 = calculate_node_metrics(G2)\n",
    "\n",
    "    # Calculate changes\n",
    "    changes = analyze_changes(G1, G2)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    k_core1 = calculate_k_core(G1)\n",
    "    k_core2 = calculate_k_core(G2)\n",
    "    edge_betweenness1 = calculate_edge_betweenness(G1)\n",
    "    edge_betweenness2 = calculate_edge_betweenness(G2)\n",
    "    node_strength1 = calculate_node_strength(G1)\n",
    "    node_strength2 = calculate_node_strength(G2)\n",
    "    assortativity1 = calculate_assortativity(G1)\n",
    "    assortativity2 = calculate_assortativity(G2)\n",
    "    local_efficiency1 = calculate_local_efficiency(G1)\n",
    "    local_efficiency2 = calculate_local_efficiency(G2)\n",
    "    robustness1 = assess_network_robustness(G1)\n",
    "    robustness2 = assess_network_robustness(G2)\n",
    "\n",
    "    # Additional analysis for incident response\n",
    "    critical_paths = identify_critical_paths(G2)\n",
    "    bridge_nodes = identify_bridge_nodes(G2)\n",
    "    segmentation = calculate_network_segmentation(G2)\n",
    "    lateral_paths = find_potential_lateral_movement(G2)\n",
    "\n",
    "    # Calculate centrality changes\n",
    "    centrality_metrics = ['Degree Centrality', 'Betweenness Centrality', \n",
    "                         'Closeness Centrality', 'Eigenvector Centrality']\n",
    "    centrality_changes = node_metrics2[centrality_metrics] - node_metrics1[centrality_metrics]\n",
    "    centrality_changes.rename(columns={\n",
    "        'Degree Centrality': 'Degree',\n",
    "        'Betweenness Centrality': 'Betweenness',\n",
    "        'Closeness Centrality': 'Closeness',\n",
    "        'Eigenvector Centrality': 'Eigenvector'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Detect anomalies\n",
    "    anomalies = centrality_changes.copy()\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    anomalies['anomaly'] = iso_forest.fit_predict(centrality_changes.values)\n",
    "\n",
    "    # Prepare DataFrames for output\n",
    "    network_changes = {k: network_metrics2[k] - network_metrics1[k] \n",
    "                      for k in network_metrics1.keys()}\n",
    "    spectral_metrics = ['Spectral Radius', 'Fiedler Eigenvalue']\n",
    "    spectral_df = pd.DataFrame({\n",
    "        'Metric': spectral_metrics,\n",
    "        'Before': [network_metrics1.get(m) for m in spectral_metrics],\n",
    "        'After': [network_metrics2.get(m) for m in spectral_metrics],\n",
    "        'Change': [network_changes.get(m) for m in spectral_metrics]\n",
    "    })\n",
    "\n",
    "    other_changes_df = pd.DataFrame({\n",
    "        'Metric': [k for k in network_changes.keys() if k not in spectral_metrics],\n",
    "        'Change': [v for k, v in network_changes.items() if k not in spectral_metrics]\n",
    "    })\n",
    "\n",
    "    structural_changes_df = pd.DataFrame({\n",
    "        'Structural Change': list(changes.keys()),\n",
    "        'Value': list(changes.values())\n",
    "    })\n",
    "\n",
    "    # Generate action items\n",
    "    action_items = generate_action_items(G1, G2, bridge_nodes, critical_paths,\n",
    "                                       segmentation, anomalies)\n",
    "\n",
    "    # Display visualizations and metrics\n",
    "    print(\"\\nGenerating incident response analysis...\")\n",
    "    visualize_network_overview(G1, G2, output_folder)\n",
    "    visualize_node_metrics_heatmap(centrality_changes, output_folder)\n",
    "    visualize_critical_infrastructure(G2, bridge_nodes, critical_paths, output_folder)\n",
    "    \n",
    "    # Display summary dashboard\n",
    "    display_summary_dashboard(action_items, changes, segmentation, anomalies)\n",
    "    \n",
    "    # Save all results\n",
    "    print(\"\\nSaving detailed analysis to output folder...\")\n",
    "    save_metrics_to_csv(spectral_df, other_changes_df, structural_changes_df, output_folder)\n",
    "    save_anomalies_to_csv(anomalies, output_folder)\n",
    "    save_additional_metrics(k_core2, assortativity1, assortativity2,\n",
    "                          local_efficiency1, local_efficiency2,\n",
    "                          robustness1, robustness2, output_folder)\n",
    "    save_action_items(action_items, output_folder)\n",
    "    create_readme(spectral_df, other_changes_df, structural_changes_df, \n",
    "                 anomalies, output_folder)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. Results saved to: {output_folder}\")\n",
    "    print(\"\\nRecommended next steps:\")\n",
    "    print(\"1. Review the action_items.md file for prioritized response actions\")\n",
    "    print(\"2. Examine critical_infrastructure.png to understand vulnerable network points\")\n",
    "    print(\"3. Check node_metrics_changes_heatmap.png for detailed node behavior changes\")\n",
    "    print(\"4. Review the README.md for a complete analysis summary\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79921e-6c29-40f6-ad0c-ffde83388459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
